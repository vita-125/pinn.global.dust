{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-01T13:51:18.174537Z",
     "start_time": "2025-10-01T13:51:18.161999Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-01T13:56:40.043853Z",
     "start_time": "2025-10-01T13:56:39.953781Z"
    }
   },
   "cell_type": "code",
   "source": [
    "FIGURE_PATH = \"C:/Users/vitas/Desktop/LE PINN/pinn.global.dust/pinn.global.dust/Code/figures/\"\n",
    "DATA_PATH = \"C:/Users/vitas/Desktop/LE PINN/pinn.global.dust/pinn.global.dust/Data/\"\n",
    "DATA_LOAD_PATH = DATA_PATH + \"original_data/\"\n",
    "INPUT_MODEL_PATH = DATA_PATH + \"processed_data/\"\n",
    "MODEL_SAVE_PATH = DATA_PATH + \"trained_models/\"\n",
    "RESULTS_PATH = DATA_PATH + \"model_results/\"\n",
    "path_to_shapefile = \"C:/Users/vitas/Desktop/LE PINN/pinn.global.dust/pinn.global.dust/ne_110m_admin_0_countries.shp\"\n",
    "world = gpd.read_file(path_to_shapefile)\n",
    "with open(\"functions_training_model.py\", 'r') as file:\n",
    "    content = file.read()\n",
    "\n",
    "# Execute the content of the .py file\n",
    "exec(content)\n",
    "df_empirical_Holocene = pd.read_csv(INPUT_MODEL_PATH + \"df_empirical_Holocene.csv\")\n",
    "df_global_grid = pd.read_csv(INPUT_MODEL_PATH + \"df_global_grid.csv\")\n",
    "df_empirical_holocene= gpd.read_file(\"C:/Users/vitas/Desktop/LE PINN/pinn.global.dust/pinn.global.dust/Data/df_empirical_holocene.geojson\")\n",
    "df_validation=gpd.read_file(\"C:/Users/vitas/Desktop/LE PINN/pinn.global.dust/pinn.global.dust/Data/df_empirical_holocene_val.geojson\")\n",
    "df_wind = pd.read_csv(INPUT_MODEL_PATH + \"df_wind.csv\", usecols=['wind', 'latitude'])\n",
    "\n",
    "latitude_wind = torch.tensor(df_wind['latitude'].values / 90, dtype=torch.float32)\n",
    "mean_wind = torch.tensor(df_wind['wind'].values / df_wind['wind'].max(), dtype=torch.float32)\n",
    "\n",
    "\n",
    "def wind_func(lat_tensor):\n",
    "    # Interpolazione lineare in PyTorch\n",
    "    return torch.interp(lat_tensor.flatten(), latitude_wind, mean_wind).unsqueeze(1)\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "598220e02a055302",
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'functions_training_model.py'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[9], line 9\u001B[0m\n\u001B[0;32m      7\u001B[0m path_to_shapefile \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mC:/Users/vitas/Desktop/LE PINN/pinn.global.dust/pinn.global.dust/ne_110m_admin_0_countries.shp\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m      8\u001B[0m world \u001B[38;5;241m=\u001B[39m gpd\u001B[38;5;241m.\u001B[39mread_file(path_to_shapefile)\n\u001B[1;32m----> 9\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfunctions_training_model.py\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m file:\n\u001B[0;32m     10\u001B[0m     content \u001B[38;5;241m=\u001B[39m file\u001B[38;5;241m.\u001B[39mread()\n\u001B[0;32m     12\u001B[0m \u001B[38;5;66;03m# Execute the content of the .py file\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:324\u001B[0m, in \u001B[0;36m_modified_open\u001B[1;34m(file, *args, **kwargs)\u001B[0m\n\u001B[0;32m    317\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m file \u001B[38;5;129;01min\u001B[39;00m {\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m}:\n\u001B[0;32m    318\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    319\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIPython won\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt let you open fd=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfile\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m by default \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    320\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    321\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124myou can use builtins\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m open.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    322\u001B[0m     )\n\u001B[1;32m--> 324\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m io_open(file, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'functions_training_model.py'"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-01T13:54:46.892115Z",
     "start_time": "2025-10-01T13:54:46.883211Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class PINN(nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super(PINN, self).__init__()\n",
    "        self.activation = nn.SiLU()\n",
    "        layer_list = []\n",
    "\n",
    "        for i in range(len(layers) - 1):\n",
    "            layer_list.append(nn.Linear(layers[i], layers[i + 1]))\n",
    "            if i < len(layers) - 2:\n",
    "                layer_list.append(self.activation)\n",
    "        self.model = nn.Sequential(*layer_list)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n"
   ],
   "id": "a58b9302b32d9010",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-01T13:54:49.822068Z",
     "start_time": "2025-10-01T13:54:49.773944Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# PDE\n",
    "D = torch.nn.Parameter(torch.tensor(1.0, requires_grad=True))\n",
    "\n",
    "\n",
    "def pde_residual(model, x, wind_func, D):\n",
    "    x.requires_grad_(True)\n",
    "    u = model(x)\n",
    "\n",
    "    grad_u = autograd.grad(u, x, grad_outputs=torch.ones_like(u), create_graph=True)[0]\n",
    "\n",
    "    # Derivate parziali\n",
    "    du_dx = grad_u[:, 0:1]\n",
    "    du_dy = grad_u[:, 1:2]\n",
    "\n",
    "    # Secondi ordini\n",
    "    d2u_dx2 = autograd.grad(du_dx, x, grad_outputs=torch.ones_like(du_dx), create_graph=True)[0][:, 0:1]\n",
    "    d2u_dy2 = autograd.grad(du_dy, x, grad_outputs=torch.ones_like(du_dy), create_graph=True)[0][:, 1:2]\n",
    "\n",
    "    # Coefficiente K in funzione della latitudine (seconda colonna di x)\n",
    "    K = wind_func(x[:, 1:2])\n",
    "\n",
    "    # Residuo PDE (esempio diffusione anisotropa)\n",
    "    residual = (-K * du_dx * (1 / torch.cos(x[:, 1:2] * np.pi / 2)) + D * (\n",
    "        (1 / (torch.cos(x[:, 1:2] * np.pi / 2) ** 2) * d2u_dx2 + d2u_dy2 - torch.tan(x[:, 1:2] * np.pi / 2) * du_dy)))\n",
    "    return residual\n"
   ],
   "id": "2cfd5f6e2667214d",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-01T13:54:53.204445Z",
     "start_time": "2025-10-01T13:54:53.119445Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === PARAMETRI DEL DOMINIO ===\n",
    "x_min, x_max = -2.0, 2.0\n",
    "y_min, y_max = -0.89, 0.89\n",
    "\n",
    "# === 1. DATI OSSERVATI ===\n",
    "# Assumi che lon/lat siano già scalati correttamente in [-2, 2] e [-0.89, 0.89]\n",
    "X_obs = torch.tensor(df_empirical_Holocene[['lon', 'lat']].values / 90, dtype=torch.float32)\n",
    "Y_obs = torch.tensor(df_empirical_Holocene['log_dep_norm'].values.reshape(-1, 1), dtype=torch.float32)\n",
    "\n",
    "# Verifica range\n",
    "print(\"Range X_obs:\", X_obs[:, 0].min().item(), X_obs[:, 0].max().item())\n",
    "print(\"Range Y_obs:\", X_obs[:, 1].min().item(), X_obs[:, 1].max().item())\n",
    "\n",
    "# === 2. PUNTI INTERNI (PDE) ===\n",
    "N_pde = 3000\n",
    "\n",
    "xy_pde = torch.cat([\n",
    "    torch.rand(N_pde, 1) * (x_max - x_min) + x_min,  # x in [x_min, x_max]\n",
    "    torch.rand(N_pde, 1) * (y_max - y_min) + y_min  # y in [y_min, y_max]\n",
    "], dim=1)\n",
    "\n",
    "# === 3. CONDIZIONI AL CONTORNO DIRICHLET ===\n",
    "\n",
    "# Nord: y = y_max\n",
    "N_bc = 200\n",
    "X_north = torch.rand(N_bc, 1) * (x_max - x_min) + x_min\n",
    "Y_north = torch.full_like(X_north, y_max)\n",
    "X_bc_north = torch.cat([X_north, Y_north], dim=1)\n",
    "Y_bc_north = torch.full((N_bc, 1), -1.0)  # Valore BC nord\n",
    "\n",
    "# Sud: y = y_min\n",
    "X_south = torch.rand(N_bc, 1) * (x_max - x_min) + x_min\n",
    "Y_south = torch.full_like(X_south, y_min)\n",
    "X_bc_south = torch.cat([X_south, Y_south], dim=1)\n",
    "Y_bc_south = torch.full((N_bc, 1), -2.0)  # Valore BC sud\n",
    "\n",
    "# === 4. CONDIZIONI PERIODICHE (x = x_min e x = x_max) ===\n",
    "N_periodic = 200\n",
    "Y_periodic = torch.linspace(y_min, y_max, N_periodic).unsqueeze(1)\n",
    "\n",
    "X_periodic_left = torch.full_like(Y_periodic, x_min)\n",
    "X_periodic_right = torch.full_like(Y_periodic, x_max)\n",
    "\n",
    "X_periodic_left = torch.cat([X_periodic_left, Y_periodic], dim=1)\n",
    "X_periodic_right = torch.cat([X_periodic_right, Y_periodic], dim=1)\n"
   ],
   "id": "e172e4d3725cc2ac",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_empirical_Holocene' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[7], line 7\u001B[0m\n\u001B[0;32m      3\u001B[0m y_min, y_max \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m0.89\u001B[39m, \u001B[38;5;241m0.89\u001B[39m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;66;03m# === 1. DATI OSSERVATI ===\u001B[39;00m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;66;03m# Assumi che lon/lat siano già scalati correttamente in [-2, 2] e [-0.89, 0.89]\u001B[39;00m\n\u001B[1;32m----> 7\u001B[0m X_obs \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor(df_empirical_Holocene[[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlon\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlat\u001B[39m\u001B[38;5;124m'\u001B[39m]]\u001B[38;5;241m.\u001B[39mvalues \u001B[38;5;241m/\u001B[39m \u001B[38;5;241m90\u001B[39m, dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mfloat32)\n\u001B[0;32m      8\u001B[0m Y_obs \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor(df_empirical_Holocene[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlog_dep_norm\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mvalues\u001B[38;5;241m.\u001B[39mreshape(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m1\u001B[39m), dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mfloat32)\n\u001B[0;32m     10\u001B[0m \u001B[38;5;66;03m# Verifica range\u001B[39;00m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'df_empirical_Holocene' is not defined"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# K in funzione della latitudine (normalizzata)\n",
    "def wind_func(lat_tensor):\n",
    "    return 1.0 + 0.5 * torch.sin(np.pi * lat_tensor)  # funzione fittizia\n"
   ],
   "id": "f3ba3746a2342660"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def total_loss(model, X_obs, Y_obs, xy_pde, X_bc1, Y_bc1,\n",
    "               X_bc2, Y_bc2, X_periodic_left, X_periodic_right,\n",
    "               wind_func, D,\n",
    "               lambda_obs=5.0, lambda_pde=10.0, lambda_bc1=0.5, lambda_bc2=0.5,\n",
    "               lambda_per0=1.0, lambda_per1=1.0):\n",
    "    # Loss osservati\n",
    "    pred_obs = model(X_obs)\n",
    "    loss_obs = nn.MSELoss()(pred_obs, Y_obs)\n",
    "\n",
    "    # PDE residual\n",
    "    res_pde = pde_residual(model, xy_pde, wind_func, D)\n",
    "    loss_pde = torch.mean(res_pde ** 2)\n",
    "\n",
    "    # BC1 Dirichlet (esempio)\n",
    "    pred_bc1 = model(X_bc1)\n",
    "    loss_bc1 = nn.MSELoss()(pred_bc1, Y_bc1)\n",
    "\n",
    "    # BC2 Dirichlet (esempio)\n",
    "    pred_bc2 = model(X_bc2)\n",
    "    loss_bc2 = nn.MSELoss()(pred_bc2, Y_bc2)\n",
    "\n",
    "    # Condizione periodica sul valore (ordine derivata 0)\n",
    "    u_left = model(X_periodic_left)\n",
    "    u_right = model(X_periodic_right)\n",
    "    loss_per0 = nn.MSELoss()(u_left, u_right)\n",
    "\n",
    "    # Condizione periodica sulla derivata (ordine derivata 1)\n",
    "    # Calcolo derivata in X_periodic_left e X_periodic_right\n",
    "    x_left = X_periodic_left.clone().detach().requires_grad_(True)\n",
    "    x_right = X_periodic_right.clone().detach().requires_grad_(True)\n",
    "\n",
    "    u_left = model(x_left)\n",
    "    u_right = model(x_right)\n",
    "\n",
    "    grad_left = autograd.grad(u_left, x_left, grad_outputs=torch.ones_like(u_left), create_graph=True)[0][:, 0:1]\n",
    "    grad_right = autograd.grad(u_right, x_right, grad_outputs=torch.ones_like(u_right), create_graph=True)[0][:, 0:1]\n",
    "\n",
    "    loss_per1 = nn.MSELoss()(grad_left, grad_right)\n",
    "\n",
    "    # Somma pesata\n",
    "    total = (lambda_obs * loss_obs + lambda_pde * loss_pde +\n",
    "             lambda_bc1 * loss_bc1 + lambda_bc2 * loss_bc2 +\n",
    "             lambda_per0 * loss_per0 + lambda_per1 * loss_per1)\n",
    "\n",
    "    return total, loss_obs.item(), loss_pde.item(), loss_bc1.item(), loss_bc2.item(), loss_per0.item(), loss_per1.item()\n"
   ],
   "id": "c2120eca8efdb7b8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# creo una griglia 1°x1°\n",
    "# Definisci i range\n",
    "lon_vals = np.arange(-180, 180, 1)  # Include 180\n",
    "lat_vals = np.arange(-90, 90, 1)  # Include 90\n",
    "\n",
    "# Crea la griglia con tutti i punti (lon, lat)\n",
    "lon_grid, lat_grid = np.meshgrid(lon_vals, lat_vals)\n",
    "lon_flat = lon_grid.flatten()\n",
    "lat_flat = lat_grid.flatten()\n",
    "\n",
    "# Crea il DataFrame\n",
    "df_global_grid_1x1 = pd.DataFrame({\n",
    "    'lon': lon_flat,\n",
    "    'lat': lat_flat\n",
    "})\n",
    "\n",
    "print(f\"Griglia creata con {len(df_global_grid_1x1)} punti\")\n",
    "\n",
    "# Opzionale: salva su CSV\n",
    "#df_global_grid_1x1.to_csv(\"global_grid_1x1.csv\", index=False)\n"
   ],
   "id": "a5b202fb92ed4152"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df_empirical_holocene= gpd.read_file(\"C:/Users/vitas/Desktop/LE PINN/pinn.global.dust/pinn.global.dust/Data/df_empirical_holocene.geojson\")\n",
    "df_validation=gpd.read_file(\"C:/Users/vitas/Desktop/LE PINN/pinn.global.dust/pinn.global.dust/Data/df_empirical_holocene_val.geojson\")\n",
    "df_training=df[~df['id'].isin(df_validation['id'])]"
   ],
   "id": "ce6aaea678f7765f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 4. Allena modello con i  pesi e salva la storia delle loss\n",
    "model = PINN(layers=[2, 32, 32, 32, 32, 32, 1])\n",
    "D = torch.nn.Parameter(torch.tensor(1.0, requires_grad=True))\n",
    "params = list(model.parameters()) + [D]\n",
    "optimizer_adam = torch.optim.Adam(params, lr=1e-4)\n",
    "#scheduler: scende da 1e-3 fino a 1e-5 in 10000 epoche\n",
    "scheduler = ReduceLROnPlateau(\n",
    "    optimizer_adam, mode='min', factor=0.5, patience=500,\n",
    "    min_lr=1e-7, )\n",
    "\n",
    "# 3. Definisci i pesi (lambda) manualmente\n",
    "lambda_obs = 20\n",
    "lambda_pde = 8\n",
    "lambda_bc1 = 3\n",
    "lambda_bc2 = 3\n",
    "lambda_per0 = 1\n",
    "lambda_per1 = 1\n",
    "\n",
    "epochs_adam = 150\n",
    "loss_history = []\n",
    "\n",
    "for epoch in range(epochs_adam):\n",
    "    optimizer_adam.zero_grad()\n",
    "    loss, l_obs, l_pde, l_bc1, l_bc2, l_per0, l_per1 = total_loss(\n",
    "        model, X_obs_train, Y_obs_train, xy_pde, X_bc_north, Y_bc_north, X_bc_south, Y_bc_south,\n",
    "        X_periodic_left, X_periodic_right, wind_func, D,\n",
    "        lambda_obs=lambda_obs, lambda_pde=lambda_pde, lambda_bc1=lambda_bc1, lambda_bc2=lambda_bc2,\n",
    "        lambda_per0=lambda_per0, lambda_per1=lambda_per1\n",
    "    )\n",
    "    loss.backward()\n",
    "    optimizer_adam.step()\n",
    "    scheduler.step(loss.item())\n",
    "    with torch.no_grad():\n",
    "        pred_val_epoch = model(X_obs_val)\n",
    "        val_loss_epoch = nn.MSELoss()(pred_val_epoch, Y_obs_val).item()\n",
    "    loss_history.append([loss.item(),\n",
    "                         lambda_obs * l_obs,\n",
    "                         lambda_pde * l_pde,\n",
    "                         lambda_bc1 * l_bc1 + lambda_bc2 * l_bc2,\n",
    "                         lambda_per0 * l_per0,\n",
    "                         lambda_per1 * l_per1,\n",
    "                         val_loss_epoch\n",
    "                         ])\n",
    "\n",
    "    if epoch % 500 == 0:\n",
    "        current_lr = optimizer_adam.param_groups[0]['lr']\n",
    "        print(\n",
    "            f\"Adam | Epoch {epoch:5d} | Total: {loss.item():.5f} | Obs: {l_obs:.5f} | PDE: {l_pde:.5f} | \" f\"BC: {(l_bc1 + l_bc2):.5f} | Per0: {l_per0:.5f} | Per1: {l_per1:.5f} | \"\n",
    "            f\"Val: {val_loss_epoch:.5f}\")\n",
    "\n"
   ],
   "id": "5e152ea456aa56f2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "1d9c7dfd443eeb2c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "950882e0b75e85f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
